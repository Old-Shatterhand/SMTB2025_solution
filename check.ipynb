{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7190620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "ROOT = Path(\"/\") / \"scratch\" / \"SCRATCH_SAS\" / \"roman\" / \"SMTB\" / \"embeddings\"\n",
    "models = [\"esm_t6\", \"esm_t12\", \"esm_t30\", \"esm_t33\", \"esm_t36\", \"esmc_300m\", \"esmc_600m\", \"ankh-base\", \"ankh-large\", \"prostt5\", \"prott5\"]  #, \"ohe\"]\n",
    "datasets = [\"fluorescence\", \"stability\", \"deeploc2\", \"deeploc2_bin\", \"scope_40_208\"]\n",
    "# datasets = [\"fluorescence\", \"stability\", \"esol\", \"deeploc2\", \"deeploc2_bin\", \"scope_40_208\", \"casp7\"]\n",
    "last = {\n",
    "    \"stability\": \"P68976\",\n",
    "    \"fluorescence\": \"P54024\",\n",
    "    # \"esol\": \"P03100\",\n",
    "    \"deeploc2\": \"P28302\",\n",
    "    \"deeploc2_bin\": \"P28302\",\n",
    "    \"scope_40_208\": \"P15176\",\n",
    "    # \"casp7\": \"70#1JAV_1_A\",\n",
    "}\n",
    "\n",
    "def model_to_depth(model):\n",
    "    if model == \"ohe\":\n",
    "        return 1\n",
    "    elif \"ankh\" in model:\n",
    "        return 49\n",
    "    elif \"t5\" in model:\n",
    "        return 25\n",
    "    elif \"esmc\" in model:\n",
    "        return 31 if \"300m\" in model else 37\n",
    "    else:\n",
    "        return int(model.split(\"t\")[-1]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89197c5",
   "metadata": {},
   "source": [
    "## Protein-Level Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942cc69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11437 out of 11437 files.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 + 13 + 31 + 34 + 37 + 31 + 37 + 49 + 49 + 25 + 25 + 1 = 339 total layers\n",
    "# 11437 files\n",
    "EMBEDDINGS_ONLY = False\n",
    "count, total = 0, 0\n",
    "prints = []\n",
    "for m, model in enumerate(models):\n",
    "    for layer in range(model_to_depth(model)):  # [:1]:\n",
    "        for d, dataset in enumerate(datasets):\n",
    "            for prefix in [\"\", \"empty_\"]:\n",
    "                # for each model, for each layer, for each dataset we have:\n",
    "                print(f\"\\r{m}-{layer}-{d} | {count}/{total}\", end=\" \"*10)\n",
    "                if not (dataset == \"deeploc2_bin\" or (ROOT / (prefix + model) / dataset / f\"layer_{layer}\" / f\"{last[dataset]}.pkl\").exists()):\n",
    "                    # if the last embedding of the protein in the dataset is missing, there are likly no embeddings\n",
    "                    # there are no embeddings for deeploc2_bin (same as deeploc2)\n",
    "                    prints.append(f\"\\rMissing embeddings: {ROOT / (prefix + model) / dataset / f'layer_{layer}'}\")\n",
    "                    continue\n",
    "                if EMBEDDINGS_ONLY:\n",
    "                    continue\n",
    "                \n",
    "                # Count Intrinsic Dimension calculations, only for trained PLMS not for empty ones  # all of them have to be past 15:00 GMT+1 11/03/25\n",
    "                if prefix == \"\":\n",
    "                    if dataset != \"deeploc2_bin\":  # no dataset statistics for deepoc2_bin\n",
    "                        for filename in [\"ids\", \"noverlap\"]:\n",
    "                            if filename.startswith(\"noverlap\") and layer == model_to_depth(model) - 1:  # no noverlap for last layer as it's an in-between-layer metric\n",
    "                                continue\n",
    "                            if dataset == \"scope_40_208\":\n",
    "                                for level in [\"superfamily\", \"fold\"]:\n",
    "                                    fpath = ROOT / (prefix + model) / dataset / f\"layer_{layer}\" / f\"{filename}_{level}_min10{'_10' if filename[0] == 'n' else ''}.csv\"\n",
    "                                    if not fpath.exists():\n",
    "                                        prints.append(f\"\\rMissing {filename}_{level}_min10{'_10' if filename[0] == 'n' else ''}.csv: {ROOT / (prefix + model) / dataset / f'layer_{layer}'}\")\n",
    "                                    else:\n",
    "                                        count += 1\n",
    "                                    total += 1\n",
    "                            else:\n",
    "                                if not (ROOT / (prefix + model) / dataset / f\"layer_{layer}\" / f\"{filename}.csv\").exists():\n",
    "                                    prints.append(f\"\\rMissing {filename} file: {ROOT / (prefix + model) / dataset / f'layer_{layer}'}\")\n",
    "                                # elif os.path.getmtime(fpath) < 1762178956.2564127:  # roughly 15:10 GMT+1 03.11.2025\n",
    "                                #     prints.append(f\"\\rOutdated {filename} file: {ROOT / model / dataset / f'layer_{layer}'}\")\n",
    "                                else:\n",
    "                                    count += 1\n",
    "                                total += 1\n",
    "\n",
    "                # Check for downstream models\n",
    "                for algo in [\"lr\", \"knn\"]:  # each model should have two models trained per layer\n",
    "                    if dataset == \"scope_40_208\":  # from the scope dataset, we have two levels of labels: superfamily and fold\n",
    "                        for level in [\"superfamily\", \"fold\"]:\n",
    "                            # TODO: Check for min_10-files\n",
    "                            count += (e := (ROOT / (prefix + model) / dataset / f\"layer_{layer}\" / f\"predictions_{algo}_{level}_min10.pkl\").exists())\n",
    "                            total += 1\n",
    "                            if not e:\n",
    "                                prints.append(f\"\\rMissing predictions_{algo}_{level}_min10.pkl: {ROOT / (prefix + model) / dataset / f'layer_{layer}'}\")\n",
    "                    else:\n",
    "                        count += (e := (ROOT / (prefix + model) / dataset / f\"layer_{layer}\" / f\"predictions_{algo}_42.pkl\").exists())\n",
    "                        total += 1\n",
    "                        if not e:\n",
    "                            prints.append(f\"\\rMissing {algo.upper()} file: {ROOT / (prefix + model) / dataset / f'layer_{layer}'}\")\n",
    "print(f\"\\rFound {count} out of {total} files.\")\n",
    "print(\"\\n\".join(sorted(prints)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f3ba1",
   "metadata": {},
   "source": [
    "## Amino-Acid Level Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691bd4e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smtb25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
